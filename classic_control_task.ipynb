{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "\n",
    "from typing import (\n",
    "    Tuple,\n",
    "    Dict,\n",
    "    Optional,\n",
    "    Iterable\n",
    ")\n",
    "from matplotlib import (\n",
    "    animation\n",
    ")\n",
    "from gym import (\n",
    "    spaces\n",
    ")\n",
    "from gym.error import (\n",
    "    DependencyNotInstalled\n",
    ")\n",
    "from pygame import (\n",
    "    gfxdraw\n",
    ")\n",
    "from IPython.display import (\n",
    "    HTML\n",
    ")\n",
    "\n",
    "class Maze(gym.Env):\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal  = (size - 1, size - 1)\n",
    "        self.maze  = self.__create_maze__(size=size)\n",
    "        self.distances = self.__compute_distance__(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {\n",
    "            0: 'UP',\n",
    "            1: 'RIGHT',\n",
    "            2: 'DOWN',\n",
    "            3: 'LEFT'\n",
    "        }\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self.__get_next_state__(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode=\"human\") -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "                        #\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between adjacent squares that are not connected)\n",
    "                        #\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "        \n",
    "        # Add the geometry of the goal square to the viewer\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + 0.5))\n",
    "        agent_col = int(scale * (self.state[1] + 0.5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * 0.6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "        )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self.__get_next_state__(state, action)\n",
    "\n",
    "        if self.shaped_rewards:\n",
    "            return -(self.distances[next_state] / self.distances.max())\n",
    "        return -float(state != self.goal)\n",
    "    \n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self.__get_next_state__(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def __get_next_state__(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError('Action value not supported: ', action)\n",
    "        \n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        \n",
    "        return state\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_maze__(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {\n",
    "            (row, col) : [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "            for row in range(size) \n",
    "            for col in range(size)\n",
    "        }\n",
    "\n",
    "        left_edges = [\n",
    "            [(row, 0), (row, -1)] \n",
    "            for row in range(size)\n",
    "        ]\n",
    "\n",
    "        right_edges = [\n",
    "            [(row, size - 1), (row, size)] \n",
    "            for row in range(size)\n",
    "        ]\n",
    "\n",
    "        upper_edges = [\n",
    "            [(0, col), (-1, col)] \n",
    "            for col in range(size)\n",
    "        ]\n",
    "\n",
    "        lower_edges = [\n",
    "            [(size - 1, col), (size, col)] \n",
    "            for col in range(size)\n",
    "        ]\n",
    "\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def __compute_distance__(goal: Tuple[int, int], maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.0\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def display_video(self, frames):\n",
    "        orig_backend = matplotlib.get_backend()\n",
    "        matplotlib.use('Agg')\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        matplotlib.use(orig_backend)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_position([0, 0, 1, 1])\n",
    "        im = ax.imshow(frames[0])\n",
    "\n",
    "        def update(frame):\n",
    "            im.set_data(frame)\n",
    "            return [im]\n",
    "        \n",
    "        anim = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "        return HTML(anim.to_html5_video())\n",
    "    \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_policy(probs_or_qvals, frame, action_meanings=None):\n",
    "    if action_meanings is None:\n",
    "        action_meanings = {0 : 'U', 1 : 'R', 2 : 'D', 3 : 'L'}\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    max_prob_actions = probs_or_qvals.argmax(axis=-1)\n",
    "    probs_copy = max_prob_actions.copy().astype(object)\n",
    "    for key in action_meanings:\n",
    "        probs_copy[probs_copy == key] = action_meanings[key]\n",
    "    sns.heatmap(max_prob_actions, annot=probs_copy, fmt='', cbar=False, cmap='coolwarm',\n",
    "                annot_kws={'weight' : 'bold', 'size' : 12}, linewidths=2, ax=axes[0])\n",
    "    axes[1].imshow(frame)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    plt.suptitle('Policy', size=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_values(state_values, frame):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    sns.heatmap(state_values, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                annot_kws={'weight' : 'bold', 'size' : 12},\n",
    "                linewidths=2, ax=axes[0])\n",
    "    axes[1].imshow(frame)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def test_agent(env:Maze, policy, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "        while not done:\n",
    "            p = policy(state)\n",
    "            if isinstance(p, np.ndarray):\n",
    "                action = np.random.choice(4, p=p)\n",
    "            else:\n",
    "                action = p\n",
    "            next_state, reward, done, extra_info = env.step(action)\n",
    "            img = env.render(mode='rgb_array')\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "    return env.display_video(frames)\n",
    "\n",
    "def quatromatrix(action_values, ax=None, triplotkw=None, tripcolorkw=None):\n",
    "    action_values = np.flipud(action_values)\n",
    "    n = 5\n",
    "    m = 5\n",
    "    a = np.array([[0, 0], [0, 1], [.5, .5], [1, 0], [1, 1]])\n",
    "    tr = np.array([[0, 1, 2], [0, 2, 3], [2, 3, 4], [1, 2, 4]])\n",
    "    A = np.zeros((n * m * 5, 2))\n",
    "    Tr = np.zeros((n * m * 4, 3))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            k = i * m + j\n",
    "            A[k * 5:(k + 1) * 5, :] = np.c_[a[:, 0] + j, a[:, 1] + i]\n",
    "            Tr[k * 4:(k + 1) * 4, :] = tr + k * 5\n",
    "    C = np.c_[action_values[:, :, 3].flatten(), action_values[:, :, 2].flatten(),\n",
    "              action_values[:, :, 1].flatten(), action_values[:, :, 0].flatten()].flatten()\n",
    "\n",
    "    ax.triplot(A[:, 0], A[:, 1], Tr, **triplotkw)\n",
    "    tripcolor = ax.tripcolor(A[:, 0], A[:, 1], Tr, facecolors=C, **tripcolorkw)\n",
    "    return tripcolor\n",
    "\n",
    "def plot_action_values(action_values):\n",
    "    text_positions = [\n",
    "        [(0.35, 4.75), (1.35, 4.75), (2.35, 4.75), (3.35, 4.75), (4.35, 4.75),\n",
    "         (0.35, 3.75), (1.35, 3.75), (2.35, 3.75), (3.35, 3.75), (4.35, 3.75),\n",
    "         (0.35, 2.75), (1.35, 2.75), (2.35, 2.75), (3.35, 2.75), (4.35, 2.75),\n",
    "         (0.35, 1.75), (1.35, 1.75), (2.35, 1.75), (3.35, 1.75), (4.35, 1.75),\n",
    "         (0.35, 0.75), (1.35, 0.75), (2.35, 0.75), (3.35, 0.75), (4.35, 0.75)],\n",
    "        [(0.6, 4.45), (1.6, 4.45), (2.6, 4.45), (3.6, 4.45), (4.6, 4.45),\n",
    "         (0.6, 3.45), (1.6, 3.45), (2.6, 3.45), (3.6, 3.45), (4.6, 3.45),\n",
    "         (0.6, 2.45), (1.6, 2.45), (2.6, 2.45), (3.6, 2.45), (4.6, 2.45),\n",
    "         (0.6, 1.45), (1.6, 1.45), (2.6, 1.45), (3.6, 1.45), (4.6, 1.45),\n",
    "         (0.6, 0.45), (1.6, 0.45), (2.6, 0.45), (3.6, 0.45), (4.6, 0.45)],\n",
    "        [(0.35, 4.15), (1.35, 4.15), (2.35, 4.15), (3.35, 4.15), (4.35, 4.15),\n",
    "         (0.35, 3.15), (1.35, 3.15), (2.35, 3.15), (3.35, 3.15), (4.35, 3.15),\n",
    "         (0.35, 2.15), (1.35, 2.15), (2.35, 2.15), (3.35, 2.15), (4.35, 2.15),\n",
    "         (0.35, 1.15), (1.35, 1.15), (2.35, 1.15), (3.35, 1.15), (4.35, 1.15),\n",
    "         (0.35, 0.15), (1.35, 0.15), (2.35, 0.15), (3.35, 0.15), (4.35, 0.15)],\n",
    "        [(0.05, 4.45), (1.05, 4.45), (2.05, 4.45), (3.05, 4.45), (4.05, 4.45),\n",
    "         (0.05, 3.45), (1.05, 3.45), (2.05, 3.45), (3.05, 3.45), (4.05, 3.45),\n",
    "         (0.05, 2.45), (1.05, 2.45), (2.05, 2.45), (3.05, 2.45), (4.05, 2.45),\n",
    "         (0.05, 1.45), (1.05, 1.45), (2.05, 1.45), (3.05, 1.45), (4.05, 1.45),\n",
    "         (0.05, 0.45), (1.05, 0.45), (2.05, 0.45), (3.05, 0.45), (4.05, 0.45)]]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    tripcolor = quatromatrix(action_values, ax=ax, triplotkw={'color' : 'k', 'lw' : 1}, tripcolorkw={'cmap' : 'coolwarm'})\n",
    "    ax.margins(0)\n",
    "    ax.set_aspect('equal')\n",
    "    fig.colorbar(tripcolor)\n",
    "\n",
    "    for j, av in enumerate(text_positions):\n",
    "        for i, (xi, yi) in enumerate(av):\n",
    "            plt.text(xi, yi, round(action_values[:, :, j].flatten()[i], 2), size=8, color='w', weight='bold')\n",
    "    \n",
    "    plt.title('Action values Q(s,a)', size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                    interval=50, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "def test_env(environment, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        frames.append(environment.render(mode=\"rgb_array\"))\n",
    "\n",
    "        while not done:\n",
    "            action = environment.action_space.sample()\n",
    "            next_state, reward, done, extra_info = environment.step(action)\n",
    "            img = environment.render(mode=\"rgb_array\")\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "\n",
    "    return display_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State space : [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
    "\n",
    "env.observation_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action space : [push_cart_to_the_left, push_cart_to_the_right]\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acrobot : Swing the bar up to a certain height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('Acrobot-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain-Car : Reach the goal from the bottom of the valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "test_env(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum : Swing it and keep it upright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('Pendulum-v1')\n",
    "#test_env(env)\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl-udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
