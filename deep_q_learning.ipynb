{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "\n",
    "from typing import (\n",
    "    Tuple,\n",
    "    Dict,\n",
    "    Optional,\n",
    "    Iterable\n",
    ")\n",
    "from matplotlib import (\n",
    "    animation\n",
    ")\n",
    "from gym import (\n",
    "    spaces\n",
    ")\n",
    "from gym.error import (\n",
    "    DependencyNotInstalled\n",
    ")\n",
    "from pygame import (\n",
    "    gfxdraw\n",
    ")\n",
    "from IPython.display import (\n",
    "    HTML\n",
    ")\n",
    "\n",
    "from tqdm import (\n",
    "    tqdm\n",
    ")\n",
    "\n",
    "class Maze(gym.Env):\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal  = (size - 1, size - 1)\n",
    "        self.maze  = self.__create_maze__(size=size)\n",
    "        self.distances = self.__compute_distance__(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {\n",
    "            0: 'UP',\n",
    "            1: 'RIGHT',\n",
    "            2: 'DOWN',\n",
    "            3: 'LEFT'\n",
    "        }\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self.__get_next_state__(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode=\"human\") -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "                        #\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between adjacent squares that are not connected)\n",
    "                        #\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "        \n",
    "        # Add the geometry of the goal square to the viewer\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + 0.5))\n",
    "        agent_col = int(scale * (self.state[1] + 0.5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * 0.6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "        )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self.__get_next_state__(state, action)\n",
    "\n",
    "        if self.shaped_rewards:\n",
    "            return -(self.distances[next_state] / self.distances.max())\n",
    "        return -float(state != self.goal)\n",
    "    \n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self.__get_next_state__(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def __get_next_state__(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError('Action value not supported: ', action)\n",
    "        \n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        \n",
    "        return state\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_maze__(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {\n",
    "            (row, col) : [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "            for row in range(size) \n",
    "            for col in range(size)\n",
    "        }\n",
    "\n",
    "        left_edges = [\n",
    "            [(row, 0), (row, -1)] \n",
    "            for row in range(size)\n",
    "        ]\n",
    "\n",
    "        right_edges = [\n",
    "            [(row, size - 1), (row, size)] \n",
    "            for row in range(size)\n",
    "        ]\n",
    "\n",
    "        upper_edges = [\n",
    "            [(0, col), (-1, col)] \n",
    "            for col in range(size)\n",
    "        ]\n",
    "\n",
    "        lower_edges = [\n",
    "            [(size - 1, col), (size, col)] \n",
    "            for col in range(size)\n",
    "        ]\n",
    "\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def __compute_distance__(goal: Tuple[int, int], maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.0\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        \n",
    "        return distances\n",
    "\n",
    "def display_video(frames):\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "    return HTML(anim.to_html5_video())\n",
    "       \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_policy(probs_or_qvals, frame, action_meanings=None):\n",
    "    if action_meanings is None:\n",
    "        action_meanings = {0 : 'U', 1 : 'R', 2 : 'D', 3 : 'L'}\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    max_prob_actions = probs_or_qvals.argmax(axis=-1)\n",
    "    probs_copy = max_prob_actions.copy().astype(object)\n",
    "    for key in action_meanings:\n",
    "        probs_copy[probs_copy == key] = action_meanings[key]\n",
    "    sns.heatmap(max_prob_actions, annot=probs_copy, fmt='', cbar=False, cmap='coolwarm',\n",
    "                annot_kws={'weight' : 'bold', 'size' : 12}, linewidths=2, ax=axes[0])\n",
    "    axes[1].imshow(frame)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    plt.suptitle('Policy', size=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_values(state_values, frame):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    sns.heatmap(state_values, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                annot_kws={'weight' : 'bold', 'size' : 12},\n",
    "                linewidths=2, ax=axes[0])\n",
    "    axes[1].imshow(frame)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def test_agent(env:gym.Env, policy, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "        while not done:\n",
    "            p = policy(state)\n",
    "            if isinstance(p, np.ndarray):\n",
    "                action = np.random.choice(4, p=p)\n",
    "            else:\n",
    "                action = p\n",
    "            next_state, reward, done, extra_info = env.step(action)\n",
    "            img = env.render(mode='rgb_array')\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "    return display_video(frames)\n",
    "\n",
    "def quatromatrix(action_values, ax=None, triplotkw=None, tripcolorkw=None):\n",
    "    action_values = np.flipud(action_values)\n",
    "    n = 5\n",
    "    m = 5\n",
    "    a = np.array([[0, 0], [0, 1], [.5, .5], [1, 0], [1, 1]])\n",
    "    tr = np.array([[0, 1, 2], [0, 2, 3], [2, 3, 4], [1, 2, 4]])\n",
    "    A = np.zeros((n * m * 5, 2))\n",
    "    Tr = np.zeros((n * m * 4, 3))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            k = i * m + j\n",
    "            A[k * 5:(k + 1) * 5, :] = np.c_[a[:, 0] + j, a[:, 1] + i]\n",
    "            Tr[k * 4:(k + 1) * 4, :] = tr + k * 5\n",
    "    C = np.c_[action_values[:, :, 3].flatten(), action_values[:, :, 2].flatten(),\n",
    "              action_values[:, :, 1].flatten(), action_values[:, :, 0].flatten()].flatten()\n",
    "\n",
    "    ax.triplot(A[:, 0], A[:, 1], Tr, **triplotkw)\n",
    "    tripcolor = ax.tripcolor(A[:, 0], A[:, 1], Tr, facecolors=C, **tripcolorkw)\n",
    "    return tripcolor\n",
    "\n",
    "def plot_action_values(action_values):\n",
    "    text_positions = [\n",
    "        [(0.35, 4.75), (1.35, 4.75), (2.35, 4.75), (3.35, 4.75), (4.35, 4.75),\n",
    "         (0.35, 3.75), (1.35, 3.75), (2.35, 3.75), (3.35, 3.75), (4.35, 3.75),\n",
    "         (0.35, 2.75), (1.35, 2.75), (2.35, 2.75), (3.35, 2.75), (4.35, 2.75),\n",
    "         (0.35, 1.75), (1.35, 1.75), (2.35, 1.75), (3.35, 1.75), (4.35, 1.75),\n",
    "         (0.35, 0.75), (1.35, 0.75), (2.35, 0.75), (3.35, 0.75), (4.35, 0.75)],\n",
    "        [(0.6, 4.45), (1.6, 4.45), (2.6, 4.45), (3.6, 4.45), (4.6, 4.45),\n",
    "         (0.6, 3.45), (1.6, 3.45), (2.6, 3.45), (3.6, 3.45), (4.6, 3.45),\n",
    "         (0.6, 2.45), (1.6, 2.45), (2.6, 2.45), (3.6, 2.45), (4.6, 2.45),\n",
    "         (0.6, 1.45), (1.6, 1.45), (2.6, 1.45), (3.6, 1.45), (4.6, 1.45),\n",
    "         (0.6, 0.45), (1.6, 0.45), (2.6, 0.45), (3.6, 0.45), (4.6, 0.45)],\n",
    "        [(0.35, 4.15), (1.35, 4.15), (2.35, 4.15), (3.35, 4.15), (4.35, 4.15),\n",
    "         (0.35, 3.15), (1.35, 3.15), (2.35, 3.15), (3.35, 3.15), (4.35, 3.15),\n",
    "         (0.35, 2.15), (1.35, 2.15), (2.35, 2.15), (3.35, 2.15), (4.35, 2.15),\n",
    "         (0.35, 1.15), (1.35, 1.15), (2.35, 1.15), (3.35, 1.15), (4.35, 1.15),\n",
    "         (0.35, 0.15), (1.35, 0.15), (2.35, 0.15), (3.35, 0.15), (4.35, 0.15)],\n",
    "        [(0.05, 4.45), (1.05, 4.45), (2.05, 4.45), (3.05, 4.45), (4.05, 4.45),\n",
    "         (0.05, 3.45), (1.05, 3.45), (2.05, 3.45), (3.05, 3.45), (4.05, 3.45),\n",
    "         (0.05, 2.45), (1.05, 2.45), (2.05, 2.45), (3.05, 2.45), (4.05, 2.45),\n",
    "         (0.05, 1.45), (1.05, 1.45), (2.05, 1.45), (3.05, 1.45), (4.05, 1.45),\n",
    "         (0.05, 0.45), (1.05, 0.45), (2.05, 0.45), (3.05, 0.45), (4.05, 0.45)]]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    tripcolor = quatromatrix(action_values, ax=ax, triplotkw={'color' : 'k', 'lw' : 1}, tripcolorkw={'cmap' : 'coolwarm'})\n",
    "    ax.margins(0)\n",
    "    ax.set_aspect('equal')\n",
    "    fig.colorbar(tripcolor)\n",
    "\n",
    "    for j, av in enumerate(text_positions):\n",
    "        for i, (xi, yi) in enumerate(av):\n",
    "            plt.text(xi, yi, round(action_values[:, :, j].flatten()[i], 2), size=8, color='w', weight='bold')\n",
    "    \n",
    "    plt.title('Action values Q(s,a)', size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import torch\n",
    "\n",
    "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def plot_tabular_cost_to_go(action_values, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    cost_to_go = -action_values.max(axis=-1)\n",
    "    plt.imshow(cost_to_go, cmap='jet')\n",
    "    plt.title(\"Estimated cost-to-go\", size=24)\n",
    "    plt.xlabel(xlabel, size=18)\n",
    "    plt.ylabel(ylabel, size=18)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xticks()\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_stats(stats):\n",
    "    rows = len(stats)\n",
    "    cols = 1\n",
    "\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
    "\n",
    "    for i, key in enumerate(stats):\n",
    "        vals = stats[key]\n",
    "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
    "        if len(stats) > 1:\n",
    "            ax[i].plot(range(len(vals)), vals)\n",
    "            ax[i].set_title(key, size=18)\n",
    "        else:\n",
    "            ax.plot(range(len(vals)), vals)\n",
    "            ax.set_title(key, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_cost_to_go(env, q_network, xlabel=None, ylabel=None):\n",
    "    highx, highy = env.observation_space.high\n",
    "    lowx, lowy = env.observation_space.low\n",
    "    X = torch.linspace(lowx, highx, 100)\n",
    "    Y = torch.linspace(lowy, highy, 100)\n",
    "    X, Y = torch.meshgrid(X, Y)\n",
    "\n",
    "    q_net_input = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
    "    Z = - q_network(q_net_input).max(dim=-1, keepdim=True)[0]\n",
    "    Z = Z.reshape(100, 100).detach().numpy()\n",
    "    X = X.numpy()\n",
    "    Y = Y.numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap='jet', linewidth=0, antialiased=False)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax.set_xlabel(xlabel, size=14)\n",
    "    ax.set_ylabel(ylabel, size=14)\n",
    "    ax.set_title(\"Estimated cost-to-go\", size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_max_q(env, q_network, xlabel=None, ylabel=None, action_labels=[]):\n",
    "    highx, highy = env.observation_space.high\n",
    "    lowx, lowy = env.observation_space.low\n",
    "    X = torch.linspace(lowx, highx, 100)\n",
    "    Y = torch.linspace(lowy, highy, 100)\n",
    "    X, Y = torch.meshgrid(X, Y)\n",
    "    q_net_input = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
    "    Z = q_network(q_net_input).argmax(dim=-1, keepdim=True)\n",
    "    Z = Z.reshape(100, 100).T.detach().numpy()\n",
    "    values = np.unique(Z.ravel())\n",
    "    values.sort()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.xlabel(xlabel, size=14)\n",
    "    plt.ylabel(ylabel, size=14)\n",
    "    plt.title(\"Optimal action\", size=18)\n",
    "\n",
    "    im = plt.imshow(Z, cmap='jet')\n",
    "    colors = [im.cmap(im.norm(value)) for value in values]\n",
    "    patches = [mpatches.Patch(color=color, label=label) for color, label in zip(colors, action_labels)]\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Import the necessary software libraries\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as ply\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.optim import (\n",
    "    AdamW\n",
    ")\n",
    "from tqdm import (\n",
    "    tqdm\n",
    ")\n",
    "\n",
    "# Prepare the environment to work with Pytorch\n",
    "\n",
    "class PreprocessEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    # Wraps env.reset()\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # [[0., 0.], [0., 0.], ...] (N x D)\n",
    "        return torch.from_numpy(state).unsqueeze(dim=0).float()\n",
    "\n",
    "    # Wraps env.step()\n",
    "    def step(self, action):\n",
    "        action = action.item()\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
    "        reward = torch.tensor(reward).view(1, -1).float()\n",
    "        done = torch.tensor(done).view(1, -1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PyVirtualEnv\\venv-rl-udemy\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x218aaa8a230>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "seed_everything(env)\n",
    "\n",
    "env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole env : State dimensions : 4, Number of actions : 2\n"
     ]
    }
   ],
   "source": [
    "state_dims = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n \n",
    "\n",
    "print(f'CartPole env : State dimensions : {state_dims}, Number of actions : {num_actions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PreprocessEnv(env)\n",
    "state = env.reset()\n",
    "action = torch.tensor(0)\n",
    "next_state, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q-network\n",
    "\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(state_dims, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, num_actions)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target network\n",
    "\n",
    "target_q_network = copy.deepcopy(q_network).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the exploratory policy : b(s)\n",
    "\n",
    "def policy(state, epsilon=0.):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        return torch.randint(num_actions, (1, 1))\n",
    "    else:\n",
    "        av = q_network(state).detach()\n",
    "        return torch.argmax(av, dim=-1, keepdim=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craete the Experience Replay Buffer\n",
    "\n",
    "class RelayMemory:\n",
    "\n",
    "    def __init__(self, capacity=1e6):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    # Insert [s, a, r, s']\n",
    "    def insert(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % int(self.capacity)\n",
    "        \n",
    "    # Sample [[s, a, r, s'], [s, a, r, s']]\n",
    "    def sample(self, batch_size):\n",
    "        assert self.can_sample(batch_size)\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # [[s,a,r,s'], [s,a,r,s'],[s,a,r,s']] -> [[s,s,s],[a,a,a],[r,r,r],[s',s',s']]\n",
    "        batch = zip(*batch)\n",
    "        return [torch.cat(items) for items in batch] # N x D\n",
    "\n",
    "\n",
    "    # can_sample -> True/False\n",
    "    def can_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size * 10\n",
    "\n",
    "    # __len__\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the algorithm\n",
    "\n",
    "def deep_q_learning(q_network, policy, episodes, alpha=1e-4, batch_size=32, gamma=0.99, epsilon=0.2):\n",
    "    optim = AdamW(q_network.parameters(), lr=alpha)\n",
    "    memory = RelayMemory()\n",
    "    stats = {'MSE Loss' : [], 'Returns' : []}\n",
    "\n",
    "    for episode in tqdm(range(1, 1 + episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0\n",
    "\n",
    "        while not done:\n",
    "            action = policy(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            memory.insert([state, action, reward, done, next_state])\n",
    "\n",
    "            if memory.can_sample(batch_size):\n",
    "                state_b, action_b, reward_b, done_b, next_state_b = memory.sample(batch_size)\n",
    "                qsa_b = q_network(state_b).gather(1, action_b)\n",
    "\n",
    "                next_qsa_b = target_q_network(next_state_b)\n",
    "                next_qsa_b = torch.max(next_qsa_b, dim=-1, keepdim=True)\n",
    "\n",
    "                target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
    "                loss = F.mse_loss(qsa_b, target_b)\n",
    "\n",
    "                q_network.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                stats['MSE Loss'].append(loss)\n",
    "\n",
    "            state = next_state\n",
    "            ep_return += reward.item()\n",
    "    \n",
    "        stats['Returns'].append(ep_return)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 32/500 [00:00<00:00, 1272.13it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mdeep_q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m, in \u001b[0;36mdeep_q_learning\u001b[1;34m(q_network, policy, episodes, alpha, batch_size, gamma, epsilon)\u001b[0m\n\u001b[0;32m     23\u001b[0m next_qsa_b \u001b[38;5;241m=\u001b[39m target_q_network(next_state_b)\n\u001b[0;32m     24\u001b[0m next_qsa_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_qsa_b, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m target_b \u001b[38;5;241m=\u001b[39m reward_b \u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m~\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_qsa_b\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(qsa_b, target_b)\n\u001b[0;32m     29\u001b[0m q_network\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "stats = deep_q_learning(q_network, policy, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl-udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
