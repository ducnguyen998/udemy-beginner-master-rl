{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic (A2C)\n",
    "\n",
    "In this notebook we are going to combine temporal difference learning (TD) with policy gradient methods./ The resulting algorithms is called Advantage Actor-Critic (A2C) and uses a one-step estimate of the return to update the policy\n",
    "\n",
    "$\n",
    "\\hat G_t = R_{t+1} + \\gamma v(S_{t+1} | w)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0 in d:\\pyvirtualenv\\venv-rl-udemy\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq gym==0.23.0\n",
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PyVirtualEnv\\venv-rl-udemy\\lib\\site-packages\\gym\\vector\\async_vector_env.py:544: UserWarning: \u001b[33mWARN: Calling `close` while waiting for a pending call to `step` to complete.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
    "\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import torch\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "\n",
    "\n",
    "class Maze(gym.Env):\n",
    "\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.maze = self._create_maze(size=size)\n",
    "        self.distances = self._compute_distances(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self._get_next_state(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
    "                        # adjacent squares that are not connected).\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
    "\n",
    "        # Add the geometry of the goal square to the viewer.\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer.\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
    "        agent_col = int(scale * (self.state[1] + .5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        if self.shaped_rewards:\n",
    "            return - (self.distances[next_state] / self.distances.max())\n",
    "        return - float(state != self.goal)\n",
    "\n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Action value not supported:\", action)\n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "                for row in range(size) for col in range(size)}\n",
    "\n",
    "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
    "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
    "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
    "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_distances(goal: Tuple[int, int],\n",
    "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                    interval=50, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def plot_stats(stats):\n",
    "    rows = len(stats)\n",
    "    cols = 1\n",
    "\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
    "\n",
    "    for i, key in enumerate(stats):\n",
    "        vals = stats[key]\n",
    "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
    "        if len(stats) > 1:\n",
    "            ax[i].plot(range(len(vals)), vals)\n",
    "            ax[i].set_title(key, size=18)\n",
    "        else:\n",
    "            ax.plot(range(len(vals)), vals)\n",
    "            ax.set_title(key, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_policy_network(env, policy, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "        while not done:\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img = env.render(mode=\"rgb_array\")\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "\n",
    "    return display_video(frames)\n",
    "\n",
    "\n",
    "def plot_action_probs(probs, labels):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(labels, probs, color ='orange')\n",
    "    plt.title(\"$\\pi(s)$\", size=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessay software libraries\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete the environment\n",
    "\n",
    "env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimensions: 6. Actions: 3\n",
      "Sample state: [ 0.99971175 -0.0240075   0.9988344  -0.04826947  0.08913929 -0.06738605]\n"
     ]
    }
   ],
   "source": [
    "dims = env.observation_space.shape[0]\n",
    "actions = env.action_space.n \n",
    "\n",
    "print(f'State dimensions: {dims}. Actions: {actions}')\n",
    "print(f'Sample state: {env.reset()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1821d4ff430>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the environment to work with Torch\n",
    "\n",
    "#class PreprocessEnv(gym.Wrapper):#\n",
    "\n",
    "#    def __init__(self, env):\n",
    "#        gym.Wrapper.__init__(self, env)#\n",
    "\n",
    "#    def reset(self):\n",
    "#        state = self.env.reset()\n",
    "#        return torch.from_numpy(state).float()#\n",
    "\n",
    "#    def step(self, actions):\n",
    "#        actions = actions.squeeze().numpy()\n",
    "#        next_state, reward, done, info = self.env.step(actions)\n",
    "#        next_state = torch.from_numpy(next_state).float()\n",
    "#        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "#        done = torch.tensor(done).unsqueeze(1)\n",
    "#        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessEnv(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        return torch.from_numpy(state).float()\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions = actions.squeeze()#.numpy()\n",
    "        next_state, reward, done, info = self.env.step(actions)\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return next_state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 8\n",
    "#parallel_env = gym.vector.make('Acrobot-v1', num_envs=num_envs)\n",
    "#seed_everything(parallel_env)\n",
    "#parallel_env = PreprocessEnv(parallel_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(\"Acrobot-v1\")\n",
    "        return PreprocessEnv(env)\n",
    "    return _thunk\n",
    "\n",
    "num_envs = 8\n",
    "parallel_env = SyncVectorEnv([make_env() for _ in range(num_envs)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the policy pi(s)\n",
    "\n",
    "actor = nn.Sequential(\n",
    "    nn.Linear(dims, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, actions),\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the value network v(s)\n",
    "\n",
    "critic = nn.Sequential(\n",
    "    nn.Linear(dims, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the algorithm\n",
    "\n",
    "def actor_critic(actor, critic, episodes, alpha=1e-4, gamma=0.99):\n",
    "    actor_optim  = AdamW(actor .parameters(), lr=1e-3)\n",
    "    critic_optim = AdamW(critic.parameters(), lr=1e-4)\n",
    "    stats = {\n",
    "        'Actor Loss' : [],\n",
    "        'Critic Loss' : [],\n",
    "        'Returns' : []\n",
    "    }\n",
    "\n",
    "    for episode in tqdm(range(1, 1 + episodes)):\n",
    "        state = parallel_env.reset()\n",
    "        state = torch.tensor(state)\n",
    "        done_b = torch.zeros((num_envs, 1), dtype=torch.bool)\n",
    "        ep_return = torch.zeros((num_envs, 1))\n",
    "        I = 1\n",
    "\n",
    "        while not done_b.all():\n",
    "            action = actor(state).multinomial(1).detach()\n",
    "            next_state, reward, done, _ = parallel_env.step(action.squeeze().cpu().numpy())\n",
    "\n",
    "            value = critic(state)\n",
    "            target = reward + ~done * gamma * critic(next_state).detach()\n",
    "            critic_loss = F.mse_loss(value, target)\n",
    "            critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "            advantage = (target - value).detach()\n",
    "            probs = actor(state)\n",
    "            log_probs = torch.log(probs + 1e-6)\n",
    "            action_log_probs = log_probs.gather(1, action)\n",
    "            entropy = - torch.sum(probs * log_probs, dim=-1, keepdim=True)\n",
    "            actor_loss = -I * action_log_probs * advantage - 0.01 * entropy\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "            ep_return += reward\n",
    "            done_b |= done\n",
    "            state = next_state\n",
    "            I = I *gamma\n",
    "\n",
    "        stats['Actor Loss'].append(actor_loss.item())\n",
    "        stats['Critic Loss'].append(critic_loss.item())\n",
    "        stats['Returns'].append(ep_return.mean().item())\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[123], line 21\u001b[0m, in \u001b[0;36mactor_critic\u001b[1;34m(actor, critic, episodes, alpha, gamma)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done_b\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     20\u001b[0m     action \u001b[38;5;241m=\u001b[39m actor(state)\u001b[38;5;241m.\u001b[39mmultinomial(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m---> 21\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     value \u001b[38;5;241m=\u001b[39m critic(state)\n\u001b[0;32m     24\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m~\u001b[39mdone \u001b[38;5;241m*\u001b[39m gamma \u001b[38;5;241m*\u001b[39m critic(next_state)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32md:\\PyVirtualEnv\\venv-rl-udemy\\lib\\site-packages\\gym\\vector\\vector_env.py:112\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each sub-environments.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    A list of auxiliary diagnostic information dicts from sub-environments.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PyVirtualEnv\\venv-rl-udemy\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:138\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[1;32m--> 138\u001b[0m     observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones[i], info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones[i]:\n\u001b[0;32m    140\u001b[0m         info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m observation\n",
      "Cell \u001b[1;32mIn[116], line 14\u001b[0m, in \u001b[0;36mPreprocessEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     12\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     13\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(next_state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 14\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     15\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(done)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_state, reward, done, info\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "stats = actor_critic(actor, critic, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show execute stats\n",
    "\n",
    "plot_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the resulting agent\n",
    "\n",
    "test_policy_network(env, actor, episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl-udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
